{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNQ+Bp16ejOjloTXtzuMN/z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9C6oi-MooeNK","executionInfo":{"status":"ok","timestamp":1762692116812,"user_tz":-330,"elapsed":20447,"user":{"displayName":"Harshada Kanawade","userId":"01855438074811417428"}},"outputId":"d2355751-e9d6-4584-8bd7-43d22e5d9915"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Context: ['i', 'love', 'learning', 'and']\n","Predicted target word: deep\n"]}],"source":["# --------------------------------------------\n","# a) Data Preparation\n","# --------------------------------------------\n","import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Dense, Flatten\n","\n","# Sample corpus\n","text = \"I love deep learning and I love neural networks\"\n","\n","# Tokenize\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","word_index = tokenizer.word_index\n","vocab_size = len(word_index) + 1\n","\n","# Convert text → sequence of numbers\n","seq = tokenizer.texts_to_sequences([text])[0]\n","\n","# --------------------------------------------\n","# b) Generate training data (Context → Target)\n","# CBOW example: context predicts center word\n","# window size = 2\n","# --------------------------------------------\n","window = 2\n","X = []\n","y = []\n","\n","for i in range(window, len(seq) - window):\n","    context = seq[i-window:i] + seq[i+1:i+window+1]\n","    target = seq[i]\n","    X.append(context)\n","    y.append(target)\n","\n","X = np.array(X)\n","y = to_categorical(y, num_classes=vocab_size)\n","\n","# --------------------------------------------\n","# c) Train Model\n","# --------------------------------------------\n","model = Sequential([\n","    Embedding(vocab_size, 8, input_length=window*2),\n","    Flatten(),\n","    Dense(32, activation='relu'),\n","    Dense(vocab_size, activation='softmax')\n","])\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(X, y, epochs=200, verbose=0)\n","\n","# --------------------------------------------\n","# d) Output\n","# --------------------------------------------\n","# Predict word from context (first example)\n","test = X[0].reshape(1, -1)\n","pred = model.predict(test, verbose=0)\n","pred_word = tokenizer.index_word[np.argmax(pred)]\n","\n","print(\"Context:\", [tokenizer.index_word[w] for w in X[0]])\n","print(\"Predicted target word:\", pred_word)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"KA2f1izqosmZ"},"execution_count":null,"outputs":[]}]}